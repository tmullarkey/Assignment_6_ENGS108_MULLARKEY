{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Thayer-ENGS108/Assignment_6_Fall2022/blob/main/assignment_6_Fall2022.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dd0qJjGWPDEY"
   },
   "source": [
    "# **ENGS 108 Fall 2022 Assignment 6**\n",
    "\n",
    "*Due November 11, 2022 at 11:59PM on Github*\n",
    "\n",
    "**Instructors:** George Cybenko\n",
    "\n",
    "**TAs:** Chase Yakaboski and Clement Nyanhongo\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## **Rules and Requirements**\n",
    "\n",
    "\n",
    "1.   You are only allowed to use Python packages that are explicity imported in \n",
    "the assignment notebook or are standard (bultin) python libraries like random, os, sys, etc, (Standard Bultin Python libraries will have a Python.org documentation). For this assignment you may use:\n",
    "  *   [numpy](https://numpy.org/doc/stable/)\n",
    "  *   [pandas](https://pandas.pydata.org/pandas-docs/stable/index.html)\n",
    "  *   [scikit-learn](https://scikit-learn.org/stable/)\n",
    "  *   [matplotlib](https://matplotlib.org/)\n",
    "  *   [tensorflow](https://www.tensorflow.org/)\n",
    "\n",
    "2.   All code must be fit into the designated code or text blocks in the assignment notebook. They are indentified by a **TODO** qualifier.\n",
    "\n",
    "3. For analytical questions that don't require code, type your answer cleanly in Markdown. For help, see the [Google Colab Markdown Guide](https://colab.research.google.com/notebooks/markdown_guide.ipynb).\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /home/t/.local/lib/python3.8/site-packages (3.7)\n",
      "Requirement already satisfied: joblib in /home/t/miniconda3/lib/python3.8/site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: click in /home/t/.local/lib/python3.8/site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: tqdm in /home/t/miniconda3/lib/python3.8/site-packages (from nltk) (4.63.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/t/.local/lib/python3.8/site-packages (from nltk) (2022.10.31)\n"
     ]
    }
   ],
   "source": [
    "!pip install --user -U nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "AD5eAz9acxe9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/t/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' Import Statements '''\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import tqdm\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1nrT0KUOtZE1"
   },
   "source": [
    "# Creating a Trump Tweet Generator!!!\n",
    "Finally it's happened. I am allowed to make my dream assignment. Trust me it's the best assignment. People tell me all the time how they wish they could have had an assignment as great as this. You will see why! But first some housekeeping... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aFcqjf9VXHfF"
   },
   "source": [
    "# Data Loading\n",
    "In this assignment we will be using a repository of Donald Trump tweets scrapped from Twitter through June 2020 from [Kaggle](https://www.kaggle.com/datasets/austinreese/trump-tweets) and will use the following code blocks to download the dataset directly to your Google Drive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0tZ4gJ5zuZjP"
   },
   "source": [
    "## Creating a Kaggle API Token\n",
    "First we will need to download an API token from Kaggle in order to download the dataset, so our first step is to create a Kaggle account if you don't already have one. (You should have done this in Assignment 4 in case this looks familiar. \n",
    "1. Create a Kaggle account by following the sign up instructions [here](https://www.kaggle.com/).\n",
    "2. Log into your Kaggle account and click your account icon on the upper righthand side. \n",
    "3. Then select **Account** from the dropdown/sidebar menu.\n",
    "4. Scroll down to the **API** section and select **Create New API Token**.\n",
    "5. This will download a JSON file called kaggle.json to your Downloads folder on your computer.\n",
    "6. Now run the following code block and when the **Browse** button appears, click it and select that kaggle.json file. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gSQSg2HkXaHF"
   },
   "source": [
    "# Run this code block after creating a Kaggle API token as instructed above.\n",
    "! pip install -q kaggle\n",
    "\n",
    "\n",
    "#Will ask you to upload kaggle.json file and remove any old ones.\n",
    "if os.path.exists('kaggle.json'):\n",
    "  os.remove('kaggle.json')\n",
    "#files.upload()\n",
    "\n",
    "# Will create the appropriate directory structure\n",
    "if not os.path.exists('/root/.kaggle'):\n",
    "  ! mkdir ~/.kaggle \n",
    "! cp kaggle.json ~/.kaggle/\n",
    "! chmod 600 ~/.kaggle/kaggle.json\n",
    "# Also we are going to make a directory called result\n",
    "if not os.path.exists('/content/results'):\n",
    "  ! mkdir /content/result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qVOhg0Rxx1HB"
   },
   "source": [
    "## Downloading the Dataset\n",
    "\n",
    "7. Now we have downloaded our Kaggle credentials we can now download the Trump Tweets Dataset (or any other Kaggle dataset for that matter) directly into our Google Drive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-r2Nya5yV3hW"
   },
   "source": [
    "! kaggle datasets download austinreese/trump-tweets\n",
    "# Will check to see if the yoga postures zip file has been unzipped and will unzip the file if not.\n",
    "! unzip trump-tweets.zip\n",
    "\n",
    "!y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EyYOK_sA1rxB"
   },
   "source": [
    "## Loading the Dataset using Pandas\n",
    "Now let's inspect the trump tweets dataset and see what we have to work with... Brace yourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "bMVBxxYS1ra1"
   },
   "outputs": [],
   "source": [
    "# Let's load in the two files that we inflated from the Kaggle download. Both realdonaldtrump.csv and trumptweets.csv are the same.\n",
    "real_donald_trump_df = pd.read_csv('realdonaldtrump.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "9joOIxr1rYGq"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>link</th>\n",
       "      <th>content</th>\n",
       "      <th>date</th>\n",
       "      <th>retweets</th>\n",
       "      <th>favorites</th>\n",
       "      <th>mentions</th>\n",
       "      <th>hashtags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1698308935</td>\n",
       "      <td>https://twitter.com/realDonaldTrump/status/169...</td>\n",
       "      <td>Be sure to tune in and watch Donald Trump on L...</td>\n",
       "      <td>2009-05-04 13:54:25</td>\n",
       "      <td>510</td>\n",
       "      <td>917</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1701461182</td>\n",
       "      <td>https://twitter.com/realDonaldTrump/status/170...</td>\n",
       "      <td>Donald Trump will be appearing on The View tom...</td>\n",
       "      <td>2009-05-04 20:00:10</td>\n",
       "      <td>34</td>\n",
       "      <td>267</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1737479987</td>\n",
       "      <td>https://twitter.com/realDonaldTrump/status/173...</td>\n",
       "      <td>Donald Trump reads Top Ten Financial Tips on L...</td>\n",
       "      <td>2009-05-08 08:38:08</td>\n",
       "      <td>13</td>\n",
       "      <td>19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1741160716</td>\n",
       "      <td>https://twitter.com/realDonaldTrump/status/174...</td>\n",
       "      <td>New Blog Post: Celebrity Apprentice Finale and...</td>\n",
       "      <td>2009-05-08 15:40:15</td>\n",
       "      <td>11</td>\n",
       "      <td>26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1773561338</td>\n",
       "      <td>https://twitter.com/realDonaldTrump/status/177...</td>\n",
       "      <td>\"My persona will never be that of a wallflower...</td>\n",
       "      <td>2009-05-12 09:07:28</td>\n",
       "      <td>1375</td>\n",
       "      <td>1945</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                                               link  \\\n",
       "0  1698308935  https://twitter.com/realDonaldTrump/status/169...   \n",
       "1  1701461182  https://twitter.com/realDonaldTrump/status/170...   \n",
       "2  1737479987  https://twitter.com/realDonaldTrump/status/173...   \n",
       "3  1741160716  https://twitter.com/realDonaldTrump/status/174...   \n",
       "4  1773561338  https://twitter.com/realDonaldTrump/status/177...   \n",
       "\n",
       "                                             content                 date  \\\n",
       "0  Be sure to tune in and watch Donald Trump on L...  2009-05-04 13:54:25   \n",
       "1  Donald Trump will be appearing on The View tom...  2009-05-04 20:00:10   \n",
       "2  Donald Trump reads Top Ten Financial Tips on L...  2009-05-08 08:38:08   \n",
       "3  New Blog Post: Celebrity Apprentice Finale and...  2009-05-08 15:40:15   \n",
       "4  \"My persona will never be that of a wallflower...  2009-05-12 09:07:28   \n",
       "\n",
       "   retweets  favorites mentions hashtags  \n",
       "0       510        917      NaN      NaN  \n",
       "1        34        267      NaN      NaN  \n",
       "2        13         19      NaN      NaN  \n",
       "3        11         26      NaN      NaN  \n",
       "4      1375       1945      NaN      NaN  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_donald_trump_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mty9AB1bd5hH"
   },
   "source": [
    "## Problem 1: Pre-Processing the Tweets\n",
    "As you may have noticed from the dataframe we just loaded, there are some special characters we need to handle. If we are making a tweet or sentence generator, we don't want to mess with special characters like commas or colons or really even captialization. So in the following section you are going to preprocess the data and strip these special characters out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C8V1hjp0rhOw"
   },
   "source": [
    "### Task 1: The Preprocess function\n",
    "In the following code block complete the preprocess function that will strip or substitute out various special characters or sequences of characters that may not be ideal when training a sentence generator. We will use the built-in re python library to do a number of substitutions, and I have given you a skeleton below, see if you can complete it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "IX8v3DzArOf6"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "REGEX_SUBS = {\n",
    "  r'\\\\': ' ',\n",
    "  r'\\n': ' ',\n",
    "  r'&': '',\n",
    "  r'RT ': '',\n",
    "  r'~': '',\n",
    "  r'#': '',\n",
    "  r'!+': '',\n",
    "  r'(http[s]?\\S+)|(\\w+\\.[A-Za-z]{2,4}\\S*)': 'link',\n",
    "  r'[*]': '',\n",
    "  r'[@]\\w+': 'user',\n",
    "  r'[:|;]': '',\n",
    "  r'[\\\\x]\\W+|\\d': '',\n",
    "  r'[\\\\x]\\W+|\\d': '',\n",
    "  r'[\\\\x]\\w+': '',\n",
    "  r'    ': ' ',\n",
    "  r'   ': ' ',\n",
    "  r'  ': ' ',\n",
    "  r'-': '',\n",
    "  r\",\": '',\n",
    "  r'\"': '',\n",
    "  r\"'\": '',\n",
    "\n",
    "}\n",
    "\n",
    "def preprocess(text):\n",
    "  # TODO: Complete the function and using the provided regular expression substitutions.\n",
    "    text = str(text)\n",
    "    for key, value in REGEX_SUBS.items():\n",
    "        text = re.sub(key, value, text).lower()\n",
    "        \n",
    "        #TODO: Do something, maybe look at the re.sub command.\n",
    "    \n",
    "    \n",
    "    \n",
    "  #TODO: Also make everything lowercase\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T_Ln5qixfHwt"
   },
   "source": [
    "### Task 2: Preprocess the dataset.\n",
    "Now that we have our preprocess function, let's preprocess all the tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "ZCY6MmC-6HhX"
   },
   "outputs": [],
   "source": [
    "rdt = np.array(real_donald_trump_df)[:,2]\n",
    "rdt_proc = []\n",
    "for i in rdt:\n",
    "    rdt_proc.append(preprocess(i))\n",
    "#TODO: Run all the Dataframe content through your preprocess function.\n",
    "\n",
    "rdt = rdt_proc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0530JEhj40JU"
   },
   "source": [
    "### Task 3: Tokenize the Data\n",
    "The next step in every Natural Language Processing task is to tokenize the data, i.e., seperate our words, special characters, etc. into separate uniquie tockens. We will be using the [Natural Language Tool Kit](https://www.nltk.org/index.html) in python to accomplish this. Study the nltk API docs and see if you can tokenize our data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "A5jwnhoU5cn7"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "preproc = []\n",
    "toke = []\n",
    "for i in rdt:\n",
    "    i = i.replace(\"/\", \"\")\n",
    "    i = i.replace(\".\",\"\")\n",
    "    i = i.replace(\")\",\"\")\n",
    "    i = i.replace(\"(\",\"\")\n",
    "    i = i.replace(\"  \",\" \")\n",
    "    i = i.replace(\"'\\'\", \"\")\n",
    "    i = i.replace('’',\"\")\n",
    "    i = i.replace(\"-\", \"\")\n",
    "    i = i.replace('–', \"\")\n",
    "    i = i.replace('‘', \"\")\n",
    "    i = i.replace(\"@\",\"\")\n",
    "    i = i.replace(\"'”'\",\"\")\n",
    "#     i = i.replace(\"''\",\"\")\n",
    "    toke.append(word_tokenize(i))\n",
    "    preproc.append(i+\"\\n\")\n",
    "    \n",
    "# TODO: Tokenize each preprocessed tweet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'be sure to tune in and watch donald trump on late night with david letterman as he presents the top ten list tonight\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preproc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UC58Uu6E3AgX"
   },
   "source": [
    "### Task 4: Build a Vocabulary\n",
    "Now that we have our data tokenized, let's build a vocabulary including beginning and ending of sentence tokens, i.e., \\<s>, \\</s> for example. At this point let's also add in these beginning and end tokens into each of our data instances. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "start = \"<s>\"\n",
    "end = \"</s>\"\n",
    "marked = []\n",
    "for i in toke:\n",
    "    new = i\n",
    "    new.insert(0,start)\n",
    "    new.append(end)\n",
    "    marked.append(new)\n",
    "flat = [i for j in marked for i in j]\n",
    "nw = len(set(flat))\n",
    "count = dict(Counter(flat))\n",
    "voc = {start:0, end:1}\n",
    "for val,key in enumerate(set(flat).symmetric_difference({start,end})):\n",
    "    voc[key] = val + 2\n",
    "voci = {val:key for key,val in voc.items()}\n",
    "dtraj = list(map(lambda x:voc[x], flat))\n",
    "countmat = np.zeros((nw,nw))\n",
    "for i in range(len(dtraj) - 1):\n",
    "    countmat[(dtraj[i+1],dtraj[i])] += 1 #column stochastic count matrix for 1 step transitions\n",
    "tmat = (countmat/countmat.sum(0)).T #normed row transition matrix\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zPvUebuqx7Zv"
   },
   "source": [
    "# Problem 1: N-gram Language Model\n",
    "In this problem, you will be building a couple n-gram language modeling and see how well just taking pure frequency counts and building a conditional probability distrbution will work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GcidjoN9yfDn"
   },
   "source": [
    "## Task 1: A 2-gram (Bigram) Model\n",
    "Recall that our goal in building a language model is to represent the conditional probability $P(w_i | w_{i-1})$ for pairs of words $w_i$ and $w_j$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "udSAfh76ypCS"
   },
   "source": [
    "### Part A: Frequencies\n",
    "Go through the encoded Trump tweets and calculate the frequencies of all words as well as all pair of words that appear next to each other in the corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "qbHlzNfazdB1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<s>': 43352,\n",
       " 'be': 6913,\n",
       " 'sure': 311,\n",
       " 'to': 19751,\n",
       " 'tune': 112,\n",
       " 'in': 11848,\n",
       " 'and': 15577,\n",
       " 'watch': 708,\n",
       " 'donald': 1829,\n",
       " 'trump': 5950,\n",
       " 'on': 7984,\n",
       " 'late': 125,\n",
       " 'night': 683,\n",
       " 'with': 5116,\n",
       " 'david': 132,\n",
       " 'letterman': 84,\n",
       " 'as': 2444,\n",
       " 'he': 3849,\n",
       " 'presents': 12,\n",
       " 'the': 34407,\n",
       " 'top': 305,\n",
       " 'ten': 31,\n",
       " 'list': 88,\n",
       " 'tonight': 752,\n",
       " '</s>': 43352,\n",
       " 'will': 6633,\n",
       " 'appearing': 14,\n",
       " 'view': 83,\n",
       " 'tomorrow': 453,\n",
       " 'morning': 364,\n",
       " 'discuss': 66,\n",
       " 'celebrity': 284,\n",
       " 'apprentice': 495,\n",
       " 'his': 2086,\n",
       " 'new': 1951,\n",
       " 'book': 413,\n",
       " 'think': 1029,\n",
       " 'like': 1723,\n",
       " 'a': 14904,\n",
       " 'champion': 178,\n",
       " 'reads': 10,\n",
       " 'financial': 76,\n",
       " 'tips': 11,\n",
       " 'show': 836,\n",
       " 'link': 9437,\n",
       " 'very': 2499,\n",
       " 'funny': 74,\n",
       " 'blog': 19,\n",
       " 'post': 191,\n",
       " 'finale': 34,\n",
       " 'lessons': 19,\n",
       " 'learned': 40,\n",
       " 'along': 159,\n",
       " 'way': 880,\n",
       " 'my': 3805,\n",
       " 'persona': 7,\n",
       " 'never': 1480,\n",
       " 'that': 5835,\n",
       " 'of': 13321,\n",
       " 'wallflower': 1,\n",
       " 'id': 137,\n",
       " 'rather': 111,\n",
       " 'build': 196,\n",
       " 'walls': 23,\n",
       " 'than': 1414,\n",
       " 'cling': 1,\n",
       " 'them': 1057,\n",
       " 'j': 144,\n",
       " 'miss': 265,\n",
       " 'usa': 505,\n",
       " 'tara': 1,\n",
       " 'conner': 2,\n",
       " 'not': 3565,\n",
       " 'fired': 204,\n",
       " 'ive': 227,\n",
       " 'always': 674,\n",
       " 'been': 1374,\n",
       " 'believer': 9,\n",
       " 'second': 221,\n",
       " 'chances': 14,\n",
       " 'says': 251,\n",
       " 'listen': 110,\n",
       " 'an': 1592,\n",
       " 'interview': 624,\n",
       " 'discussing': 269,\n",
       " 'strive': 7,\n",
       " 'for': 9947,\n",
       " 'wholeness': 2,\n",
       " 'keep': 662,\n",
       " 'your': 2667,\n",
       " 'sense': 111,\n",
       " 'wonder': 139,\n",
       " 'intact': 12,\n",
       " 'enter': 41,\n",
       " 'signed': 111,\n",
       " 'keychain': 3,\n",
       " 'contest': 21,\n",
       " 'when': 1361,\n",
       " 'achiever': 1,\n",
       " 'achieves': 1,\n",
       " 'its': 1377,\n",
       " 'plateau': 13,\n",
       " 'beginning': 134,\n",
       " 'dont': 1504,\n",
       " 'afraid': 80,\n",
       " 'being': 989,\n",
       " 'unique': 13,\n",
       " 'best': 1111,\n",
       " 'self': 40,\n",
       " 'we': 4717,\n",
       " 'win': 872,\n",
       " 'our': 4562,\n",
       " 'lives': 168,\n",
       " 'by': 3023,\n",
       " 'having': 281,\n",
       " 'champions': 23,\n",
       " 'each': 110,\n",
       " 'moment': 41,\n",
       " 'these': 459,\n",
       " 'dayswe': 1,\n",
       " 'could': 545,\n",
       " 'all': 3229,\n",
       " 'use': 278,\n",
       " 'little': 243,\n",
       " 'power': 189,\n",
       " 'trumpative': 1,\n",
       " 'thinking': 144,\n",
       " 'know': 899,\n",
       " 'you': 9717,\n",
       " 'precipice': 2,\n",
       " 'something': 273,\n",
       " 'great': 6589,\n",
       " 'read': 364,\n",
       " 'appeared': 6,\n",
       " 'york': 417,\n",
       " 'times': 323,\n",
       " 'magazine': 94,\n",
       " 'it': 5670,\n",
       " 'fast': 292,\n",
       " 'short': 83,\n",
       " 'direct': 23,\n",
       " 'whatever': 74,\n",
       " 'is': 12809,\n",
       " 'forget': 127,\n",
       " 'did': 832,\n",
       " 'facebook': 61,\n",
       " '?': 3767,\n",
       " 'become': 233,\n",
       " 'fan': 132,\n",
       " 'today': 1139,\n",
       " 'higher': 113,\n",
       " 'opposition': 35,\n",
       " 'comfort': 10,\n",
       " 'zone': 17,\n",
       " 'e': 2085,\n",
       " 'from': 2224,\n",
       " 'have': 4599,\n",
       " 'call': 355,\n",
       " 'quits': 7,\n",
       " 'moving': 98,\n",
       " 'forward': 545,\n",
       " 'check': 153,\n",
       " 'out': 1880,\n",
       " 'trumps': 212,\n",
       " 'igoogle': 1,\n",
       " 'showcase': 5,\n",
       " 'page': 88,\n",
       " '“': 3247,\n",
       " 'if': 1920,\n",
       " 'problems': 160,\n",
       " 'youre': 477,\n",
       " 'pretending': 13,\n",
       " 'or': 1485,\n",
       " 'run': 1173,\n",
       " 'own': 295,\n",
       " 'business': 557,\n",
       " '”': 3236,\n",
       " 'last': 918,\n",
       " 'week': 291,\n",
       " 'birthday': 211,\n",
       " 'send': 114,\n",
       " 'him': 1057,\n",
       " 'bday': 9,\n",
       " 'wishes': 62,\n",
       " 'here': 356,\n",
       " 'thanks': 2142,\n",
       " 'thoughtful': 1,\n",
       " 're': 119,\n",
       " 'fb': 1,\n",
       " 'vanity': 18,\n",
       " 'urls': 1,\n",
       " 'sf': 2,\n",
       " 'chronicle': 2,\n",
       " 'beckham': 1,\n",
       " 'was': 3428,\n",
       " 'one': 1570,\n",
       " 'first': 775,\n",
       " 'britney': 1,\n",
       " 'spears': 1,\n",
       " 'wishing': 32,\n",
       " 'happy': 514,\n",
       " 'fathers': 22,\n",
       " 'day': 809,\n",
       " 'dads': 4,\n",
       " 'there': 1299,\n",
       " 'are': 5658,\n",
       " 'everyday': 26,\n",
       " 'life': 323,\n",
       " 'every': 462,\n",
       " 'commercialfree': 1,\n",
       " 'wwe': 57,\n",
       " 'raw': 6,\n",
       " 'does': 467,\n",
       " 'big': 1682,\n",
       " 'rating': 103,\n",
       " 'michael': 134,\n",
       " 'jackson': 27,\n",
       " 'friend': 217,\n",
       " 'spectacular': 75,\n",
       " 'entertainer': 4,\n",
       " 'devastating': 23,\n",
       " 'loss': 56,\n",
       " 'powerful': 87,\n",
       " 'frank': 21,\n",
       " 'about': 1851,\n",
       " 'economy': 471,\n",
       " 'greta': 125,\n",
       " 'van': 30,\n",
       " 'susterens': 3,\n",
       " 'record': 503,\n",
       " 'heres': 36,\n",
       " 'safe': 186,\n",
       " 'independence': 23,\n",
       " 'enjoy': 364,\n",
       " 'aware': 18,\n",
       " 'things': 497,\n",
       " 'seem': 58,\n",
       " 'ine': 11,\n",
       " 'because': 808,\n",
       " 'they': 3555,\n",
       " 'can': 1784,\n",
       " 'step': 67,\n",
       " 'towards': 23,\n",
       " 'innovation': 21,\n",
       " 'backs': 15,\n",
       " 'randal': 1,\n",
       " 'pinkett': 1,\n",
       " 'nj': 33,\n",
       " 'lieutenant': 2,\n",
       " 'governor': 296,\n",
       " 'congrats': 162,\n",
       " 'winners': 52,\n",
       " 'around': 240,\n",
       " 'world': 799,\n",
       " 'who': 2166,\n",
       " 'entered': 10,\n",
       " 'bookkeychain': 1,\n",
       " 'books': 74,\n",
       " 'summer': 24,\n",
       " 'reading': 100,\n",
       " 'at': 4180,\n",
       " 'university': 83,\n",
       " 'browse': 1,\n",
       " 'success': 406,\n",
       " 'thats': 411,\n",
       " 'utilize': 2,\n",
       " 'develop': 20,\n",
       " 'capability': 7,\n",
       " 'ivanka': 61,\n",
       " 'now': 2020,\n",
       " 'twitter': 174,\n",
       " 'follow': 131,\n",
       " 'her': 779,\n",
       " 'ivankatrump': 193,\n",
       " 'terrific': 115,\n",
       " 'weekend': 131,\n",
       " 'recent': 61,\n",
       " 'appearance': 36,\n",
       " 'universe': 86,\n",
       " 'competition': 45,\n",
       " 'live': 336,\n",
       " 'bahamas': 13,\n",
       " 'sunday': 159,\n",
       " 'user': 3258,\n",
       " 'et': 78,\n",
       " 'nbc': 318,\n",
       " 'reminder': 5,\n",
       " 'est': 62,\n",
       " 'bids': 2,\n",
       " 'buy': 163,\n",
       " 'oreo': 2,\n",
       " 'double': 58,\n",
       " 'stuf': 1,\n",
       " 'racing': 2,\n",
       " 'league': 47,\n",
       " 'more': 1839,\n",
       " 'hysterical': 1,\n",
       " 'dsrl': 1,\n",
       " 'videos': 11,\n",
       " 'featuring': 27,\n",
       " 'plus': 84,\n",
       " 'golden': 26,\n",
       " 'lick': 1,\n",
       " 'race': 169,\n",
       " 'sweepstakes': 1,\n",
       " 'lot': 276,\n",
       " 'people': 2657,\n",
       " 'imagination': 21,\n",
       " 'but': 2167,\n",
       " 'cant': 847,\n",
       " 'eyou': 1,\n",
       " 'what': 2026,\n",
       " 'has': 3112,\n",
       " 'say': 583,\n",
       " 'daughter': 48,\n",
       " 'ivankas': 6,\n",
       " 'upcoming': 46,\n",
       " 'card': 42,\n",
       " 'video': 147,\n",
       " 'sharing': 13,\n",
       " 'advice': 95,\n",
       " 'entrepreneurial': 9,\n",
       " 'women': 231,\n",
       " 'gma': 20,\n",
       " 'hear': 239,\n",
       " 'gov': 52,\n",
       " 'spending': 190,\n",
       " 'banks': 43,\n",
       " 'ta': 374,\n",
       " 'wneil': 1,\n",
       " 'cavuto': 9,\n",
       " 'jareds': 1,\n",
       " 'wedding': 20,\n",
       " 'make': 1553,\n",
       " 'beautiful': 404,\n",
       " 'couple': 34,\n",
       " 'im': 703,\n",
       " 'proud': 289,\n",
       " 'father': 73,\n",
       " 'work': 853,\n",
       " 'begun': 26,\n",
       " 'ahead': 104,\n",
       " 'schedule': 41,\n",
       " 'greatest': 289,\n",
       " 'golf': 523,\n",
       " 'course': 441,\n",
       " 'history': 482,\n",
       " 'international': 152,\n",
       " 'scotland': 204,\n",
       " 'partners': 26,\n",
       " 'tv': 188,\n",
       " 'reality': 81,\n",
       " 'series': 34,\n",
       " 'entitled': 17,\n",
       " 'omarosas': 4,\n",
       " 'ultimate': 27,\n",
       " 'merger': 8,\n",
       " 'yours': 40,\n",
       " 'bountiful': 1,\n",
       " 'thanksgiving': 37,\n",
       " 'tower': 262,\n",
       " 'chicago': 195,\n",
       " 'ranked': 14,\n",
       " 'th': 469,\n",
       " 'tallest': 8,\n",
       " 'building': 235,\n",
       " 'council': 46,\n",
       " 'tall': 10,\n",
       " 'buildings': 31,\n",
       " 'urban': 6,\n",
       " 'habitat': 1,\n",
       " 'everyone': 451,\n",
       " 'wonderful': 427,\n",
       " 'holiday': 48,\n",
       " 'healthy': 30,\n",
       " 'prosperous': 15,\n",
       " 'year': 605,\n",
       " 'lets': 263,\n",
       " 'returns': 36,\n",
       " 'pm': 673,\n",
       " 'etpt': 1,\n",
       " 'outstanding': 63,\n",
       " 'celebrities': 20,\n",
       " 'season': 282,\n",
       " 'should': 1674,\n",
       " 'yet': 281,\n",
       " 'tycoon': 6,\n",
       " 'app': 15,\n",
       " 'iphone': 18,\n",
       " 'ipod': 3,\n",
       " 'touch': 109,\n",
       " '$': 1016,\n",
       " 'priceless': 4,\n",
       " 'i': 9267,\n",
       " 'saw': 124,\n",
       " 'lady': 65,\n",
       " 'gaga': 3,\n",
       " 'she': 862,\n",
       " 'fantastic': 384,\n",
       " 'channel': 24,\n",
       " 'host': 79,\n",
       " 'match': 33,\n",
       " 'la': 51,\n",
       " 'ca': 27,\n",
       " 'mark': 157,\n",
       " 'wahlberg': 1,\n",
       " 'vs': 47,\n",
       " 'kevin': 69,\n",
       " 'dillon': 2,\n",
       " 'superbowl': 14,\n",
       " 'american': 879,\n",
       " 'tradition': 10,\n",
       " 'colts': 5,\n",
       " 'saints': 2,\n",
       " 'already': 294,\n",
       " 'may': 292,\n",
       " 'team': 218,\n",
       " 'cnn': 556,\n",
       " 'internationals': 1,\n",
       " 'connect': 9,\n",
       " 'connector': 1,\n",
       " 'submit': 10,\n",
       " 'questions': 115,\n",
       " 'final': 105,\n",
       " 'episode': 88,\n",
       " 'jay': 42,\n",
       " 'leno': 9,\n",
       " 'deliver': 59,\n",
       " 'special': 228,\n",
       " 'message': 84,\n",
       " 'so': 2782,\n",
       " 'wife': 124,\n",
       " 'melania': 133,\n",
       " 'launch': 32,\n",
       " 'jewelry': 10,\n",
       " 'line': 93,\n",
       " 'debut': 12,\n",
       " 'qvc': 21,\n",
       " 'april': 40,\n",
       " 'andrea': 3,\n",
       " 'bocelli': 1,\n",
       " 'useralago': 1,\n",
       " 'many': 1434,\n",
       " 'entertainment': 14,\n",
       " 'long': 497,\n",
       " 'palm': 69,\n",
       " 'beach': 88,\n",
       " 'twohour': 1,\n",
       " 'premiere': 32,\n",
       " 'this': 3768,\n",
       " 'march': 109,\n",
       " 'see': 1075,\n",
       " 'then': 470,\n",
       " 'off': 382,\n",
       " 'start': 306,\n",
       " 'swept': 4,\n",
       " 'hour': 60,\n",
       " 'key': 59,\n",
       " 'demographic': 3,\n",
       " 'olympic': 21,\n",
       " 'gold': 59,\n",
       " 'medalist': 2,\n",
       " 'evan': 4,\n",
       " 'lysacek': 2,\n",
       " 'just': 2677,\n",
       " 'left': 366,\n",
       " 'office': 360,\n",
       " 'town': 43,\n",
       " 'wanted': 143,\n",
       " 'meet': 136,\n",
       " 'mehes': 1,\n",
       " 'fanastic': 1,\n",
       " 'guy': 370,\n",
       " 'true': 881,\n",
       " 'am': 1271,\n",
       " 'announce': 106,\n",
       " 'theoriginal': 1,\n",
       " 'which': 760,\n",
       " 'offer': 62,\n",
       " 'job': 1046,\n",
       " 'opportunities': 48,\n",
       " 'those': 366,\n",
       " 'needis': 1,\n",
       " 'coming': 383,\n",
       " 'back': 1203,\n",
       " 'weeks': 112,\n",
       " 'set': 156,\n",
       " 'stage': 53,\n",
       " 'even': 1015,\n",
       " 'hotel': 280,\n",
       " 'collection': 63,\n",
       " 'currently': 40,\n",
       " 'nominated': 26,\n",
       " 'conde': 8,\n",
       " 'nast': 9,\n",
       " 'traveler': 5,\n",
       " 'readers': 11,\n",
       " 'choice': 138,\n",
       " 'awards': 40,\n",
       " 'travel': 59,\n",
       " 'leisure': 3,\n",
       " 'original': 37,\n",
       " 'backdo': 1,\n",
       " 'takes': 92,\n",
       " 'ne': 506,\n",
       " 'casting': 11,\n",
       " 'details': 31,\n",
       " 'nyc': 163,\n",
       " 'thisthursday': 1,\n",
       " 'information': 139,\n",
       " 'need': 1051,\n",
       " 'go': 960,\n",
       " 'open': 316,\n",
       " 'look': 677,\n",
       " 'seeing': 126,\n",
       " 'todays': 129,\n",
       " 'drew': 21,\n",
       " 'thousands': 151,\n",
       " 'eager': 2,\n",
       " 'applicants': 2,\n",
       " 'impressive': 28,\n",
       " 'group': 84,\n",
       " 'enjoyed': 73,\n",
       " 'meeting': 314,\n",
       " 'weve': 68,\n",
       " 'got': 700,\n",
       " 'some': 538,\n",
       " 'candidates': 115,\n",
       " 'attended': 15,\n",
       " 'skating': 8,\n",
       " 'stars': 26,\n",
       " 'gala': 17,\n",
       " 'wollman': 8,\n",
       " 'rink': 23,\n",
       " 'central': 52,\n",
       " 'park': 67,\n",
       " 'stay': 256,\n",
       " 'tuned': 72,\n",
       " 'part': 183,\n",
       " 'scottish': 39,\n",
       " 'fashion': 17,\n",
       " 'benefits': 28,\n",
       " 'veterans': 139,\n",
       " 'dressed': 8,\n",
       " 'kilt': 2,\n",
       " 'cohosted': 2,\n",
       " 'sir': 128,\n",
       " 'sean': 33,\n",
       " 'connery': 1,\n",
       " 'bestselling': 4,\n",
       " 'available': 64,\n",
       " 'paperback': 3,\n",
       " 'inspiring': 31,\n",
       " 'entertaining': 24,\n",
       " 'soho': 14,\n",
       " 'opens': 24,\n",
       " 'friday': 90,\n",
       " 'downtown': 19,\n",
       " 'unlike': 48,\n",
       " 'anything': 282,\n",
       " 'else': 223,\n",
       " 'visit': 133,\n",
       " 'soon': 455,\n",
       " 'fabulous': 48,\n",
       " 'adventure': 4,\n",
       " 'looking': 603,\n",
       " 'yankees': 93,\n",
       " 'opening': 77,\n",
       " 'before': 542,\n",
       " 'kids': 48,\n",
       " 'places': 49,\n",
       " 'place': 371,\n",
       " 'motto': 8,\n",
       " 'police': 135,\n",
       " 'athletic': 7,\n",
       " 'organization': 50,\n",
       " 'support': 456,\n",
       " 'jerome': 8,\n",
       " 'bettis': 1,\n",
       " 'bus': 17,\n",
       " 'pittsburgh': 24,\n",
       " 'steelers': 1,\n",
       " 'play': 189,\n",
       " 'intl': 69,\n",
       " 'clubpalm': 1,\n",
       " 'against': 575,\n",
       " 'julius': 2,\n",
       " 'erving': 1,\n",
       " 'dr': 45,\n",
       " 'marriage': 12,\n",
       " 'ref': 2,\n",
       " 'onthursday': 1,\n",
       " 'nbcim': 1,\n",
       " 'panel': 18,\n",
       " 'gloria': 4,\n",
       " 'estefan': 1,\n",
       " 'adam': 100,\n",
       " 'carolla': 2,\n",
       " 'idiot': 15,\n",
       " 'broadway': 7,\n",
       " 'amazing': 598,\n",
       " 'theatrical': 1,\n",
       " 'cbs': 61,\n",
       " 'fun': 153,\n",
       " 'put': 373,\n",
       " 'calendar': 4,\n",
       " 'las': 105,\n",
       " 'vegas': 129,\n",
       " 'ill': 254,\n",
       " 'theretune': 1,\n",
       " 'thoughts': 131,\n",
       " 'prayers': 86,\n",
       " 'remain': 58,\n",
       " 'bret': 22,\n",
       " 'michaels': 13,\n",
       " 'family': 302,\n",
       " 'speedy': 8,\n",
       " 'recovery': 62,\n",
       " 'larry': 30,\n",
       " 'king': 77,\n",
       " 'conversation': 72,\n",
       " 'introduce': 6,\n",
       " 'timepieces': 6,\n",
       " 'where': 498,\n",
       " 'debuting': 2,\n",
       " 'pageant': 80,\n",
       " 'hosted': 39,\n",
       " 'curtis': 6,\n",
       " 'stone': 42,\n",
       " 'natalie': 6,\n",
       " 'morales': 3,\n",
       " 'also': 576,\n",
       " 'country': 1872,\n",
       " 'superstar': 5,\n",
       " 'trace': 13,\n",
       " 'adkins': 1,\n",
       " 'pop': 3,\n",
       " 'rock': 51,\n",
       " 'sensation': 3,\n",
       " 'boys': 20,\n",
       " 'girls': 15,\n",
       " 'providing': 10,\n",
       " 'backstage': 7,\n",
       " 'commentary': 19,\n",
       " 'comedic': 1,\n",
       " 'motherdaughter': 1,\n",
       " 'duo': 3,\n",
       " 'joan': 52,\n",
       " 'melissa': 4,\n",
       " 'rivers': 43,\n",
       " 'lineup': 9,\n",
       " 'were': 1023,\n",
       " 'progress': 93,\n",
       " 'full': 230,\n",
       " 'congratulations': 510,\n",
       " 'rima': 3,\n",
       " 'fakih': 2,\n",
       " 'represent': 37,\n",
       " 'us': 2224,\n",
       " 'well': 697,\n",
       " 'up': 1610,\n",
       " 'whats': 136,\n",
       " 'happeningfrom': 1,\n",
       " 'jewlery': 1,\n",
       " 'free': 166,\n",
       " 'tickets': 95,\n",
       " 'latest': 44,\n",
       " 'update': 14,\n",
       " 'hes': 401,\n",
       " 'making': 394,\n",
       " 'effort': 69,\n",
       " 'attend': 19,\n",
       " 'sundayso': 1,\n",
       " 'whos': 35,\n",
       " 'pick': 66,\n",
       " 'bretmichaels': 17,\n",
       " 'hollyrpeete': 4,\n",
       " 'vote': 1172,\n",
       " 'brets': 1,\n",
       " 'caught': 112,\n",
       " 'holly': 6,\n",
       " 'both': 327,\n",
       " 'ratings': 352,\n",
       " 'had': 947,\n",
       " 'time': 1626,\n",
       " 'spent': 148,\n",
       " 'several': 8,\n",
       " 'years': 974,\n",
       " 'right': 744,\n",
       " 'visiting': 35,\n",
       " 'over': 1064,\n",
       " 'sites': 16,\n",
       " 'absolutely': 151,\n",
       " 'north': 383,\n",
       " 'coast': 50,\n",
       " 'spectacularthe': 1,\n",
       " 'sea': 12,\n",
       " 'sand': 12,\n",
       " 'dunes': 11,\n",
       " 'rolling': 28,\n",
       " 'bluffswe': 1,\n",
       " 'walked': 15,\n",
       " 'theyre': 114,\n",
       " 'how': 1154,\n",
       " 'geomorphology': 1,\n",
       " 'study': 42,\n",
       " 'movement': 82,\n",
       " 'landforms': 1,\n",
       " 'trip': 66,\n",
       " 'memorial': 49,\n",
       " 'thought': 195,\n",
       " 'done': 841,\n",
       " 'much': 1221,\n",
       " 'freedom': 106,\n",
       " 'turning': 48,\n",
       " 'table': 23,\n",
       " 'saturday': 118,\n",
       " 'interviewing': 8,\n",
       " 'honor': 391,\n",
       " 'anniversary': 55,\n",
       " 'checking': 16,\n",
       " 'tngc': 1,\n",
       " 'westchester': 9,\n",
       " 'guest': 51,\n",
       " 'trade': 483,\n",
       " 'kingsthings': 5,\n",
       " 'clips': 6,\n",
       " 'honored': 96,\n",
       " 'chosen': 17,\n",
       " 'gray': 3,\n",
       " 'their': 1428,\n",
       " 'ny': 133,\n",
       " 'ride': 18,\n",
       " 'fame': 54,\n",
       " 'campaign': 514,\n",
       " 'ribbon': 8,\n",
       " 'cutting': 58,\n",
       " 'ceremony': 31,\n",
       " 'front': 66,\n",
       " 'lines': 34,\n",
       " 'site': 43,\n",
       " 'rode': 3,\n",
       " 'me': 2470,\n",
       " 'yesterday': 322,\n",
       " 'wsj': 60,\n",
       " 'covers': 20,\n",
       " 'dinner': 89,\n",
       " 'quattro': 1,\n",
       " 'hottest': 13,\n",
       " 'restaurants': 29,\n",
       " 'city': 210,\n",
       " 'hotels': 58,\n",
       " 'anywhere': 56,\n",
       " 'starring': 3,\n",
       " 'only': 1243,\n",
       " 'omarosa': 70,\n",
       " 'twelve': 7,\n",
       " 'brave': 87,\n",
       " 'bachelors': 2,\n",
       " 'relief': 34,\n",
       " 'telethon': 3,\n",
       " 'tonights': 54,\n",
       " 'two': 514,\n",
       " 'gulf': 7,\n",
       " 'million': 288,\n",
       " 'raised': 67,\n",
       " 'hours': 93,\n",
       " 'eric': 86,\n",
       " 'foundation': 51,\n",
       " 'st': 125,\n",
       " 'jude': 6,\n",
       " 'childrens': 17,\n",
       " 'research': 33,\n",
       " 'hospital': 26,\n",
       " 'trading': 14,\n",
       " 'shots': 21,\n",
       " 'trumpa': 1,\n",
       " 'article': 95,\n",
       " 'wall': 473,\n",
       " 'street': 96,\n",
       " 'journal': 53,\n",
       " 'begin': 52,\n",
       " 'thankful': 16,\n",
       " 'due': 84,\n",
       " 'popular': 70,\n",
       " 'demand': 81,\n",
       " 'rebroadcast': 6,\n",
       " 'june': 74,\n",
       " 'monday': 153,\n",
       " 'july': 44,\n",
       " 'edition': 6,\n",
       " 'thursdays': 4,\n",
       " 'fall': 68,\n",
       " 'etim': 1,\n",
       " 'putting': 90,\n",
       " 'george': 102,\n",
       " 'steinbrenner': 11,\n",
       " 'legend': 32,\n",
       " 'anyone': 247,\n",
       " 'lost': 304,\n",
       " 'truly': 264,\n",
       " 'man': 588,\n",
       " 'visited': 34,\n",
       " 'courses': 68,\n",
       " 'weekendthis': 1,\n",
       " 'onelink': 4,\n",
       " 'anotherlink': 1,\n",
       " 'another': 545,\n",
       " 'accept': 36,\n",
       " 'hollyrod': 1,\n",
       " 'humanitarian': 28,\n",
       " 'award': 81,\n",
       " 'robinson': 3,\n",
       " 'peete': 1,\n",
       " 'newest': 9,\n",
       " 'helping': 93,\n",
       " 'serta': 12,\n",
       " 'counting': 15,\n",
       " 'sheep': 7,\n",
       " 'get': 1958,\n",
       " 'contestlink': 1,\n",
       " 'august': 38,\n",
       " 'cohost': 4,\n",
       " 'performing': 9,\n",
       " 'mandalay': 1,\n",
       " 'bay': 34,\n",
       " 'resort': 70,\n",
       " 'casino': 5,\n",
       " 'telemundo': 7,\n",
       " 'orianthi': 1,\n",
       " 'john': 342,\n",
       " 'roots': 8,\n",
       " 'calenders': 1,\n",
       " 'rd': 108,\n",
       " 'september': 44,\n",
       " 'erics': 1,\n",
       " 'sept': 8,\n",
       " 'event': 154,\n",
       " 'held': 70,\n",
       " 'national': 588,\n",
       " 'club': 168,\n",
       " 'webisode': 2,\n",
       " 'favorite': 143,\n",
       " 'hire': 23,\n",
       " 'click': 5,\n",
       " 'would': 1992,\n",
       " 'vegaslink': 2,\n",
       " 'thursday': 72,\n",
       " 'four': 99,\n",
       " 'days': 196,\n",
       " 'until': 242,\n",
       " 'nbcit': 2,\n",
       " 'going': 1245,\n",
       " 'hotter': 2,\n",
       " 'jimena': 1,\n",
       " 'navarrete': 2,\n",
       " 'raked': 2,\n",
       " 'golfing': 7,\n",
       " 'bedminster': 42,\n",
       " 'went': 200,\n",
       " 'game': 210,\n",
       " 'bill': 352,\n",
       " 'oreillywe': 1,\n",
       " 'watching': 346,\n",
       " 'tonightbe': 1,\n",
       " 'sertas': 1,\n",
       " 'invited': 10,\n",
       " 'caroline': 2,\n",
       " 'wozniacki': 1,\n",
       " 'sit': 31,\n",
       " 'boduring': 1,\n",
       " 'shes': 87,\n",
       " 'theres': 98,\n",
       " 'civilian': 7,\n",
       " 'version': 21,\n",
       " 'air': 116,\n",
       " 'opportunity': 85,\n",
       " 'annual': 35,\n",
       " 'charity': 107,\n",
       " 'outing': 4,\n",
       " 'melanias': 6,\n",
       " 'action': 141,\n",
       " 'packed': 53,\n",
       " 'sizzle': 1,\n",
       " 'reel': 1,\n",
       " 'dog': 46,\n",
       " 'whisperer': 1,\n",
       " 'nights': 72,\n",
       " 'weekly': 24,\n",
       " 'presented': 13,\n",
       " 'doctor': 23,\n",
       " 'administration': 306,\n",
       " 'honoris': 1,\n",
       " 'causa': 1,\n",
       " 'robert': 110,\n",
       " 'gordon': 6,\n",
       " 'aberdeen': 55,\n",
       " 'internatonal': 1,\n",
       " 'linksscotland': 1,\n",
       " 'progressing': 2,\n",
       " 'beautifully': 7,\n",
       " 'nbcits': 1,\n",
       " 'wont': 308,\n",
       " 'hr': 13,\n",
       " 'solutions': 31,\n",
       " 'company': 94,\n",
       " 'polled': 6,\n",
       " 'employed': 10,\n",
       " 'adults': 9,\n",
       " 'find': 224,\n",
       " 'ideal': 12,\n",
       " 'bosses': 23,\n",
       " 'unpredictable': 4,\n",
       " 'prepared': 49,\n",
       " 'sensational': 6,\n",
       " 'talked': 38,\n",
       " 'onetough': 1,\n",
       " 'surprising': 8,\n",
       " 'pmnbc': 1,\n",
       " 'variety': 5,\n",
       " 'topics': 17,\n",
       " 'susteren': 15,\n",
       " 'fonews': 54,\n",
       " 'want': 1122,\n",
       " 'surprise': 84,\n",
       " 'slightly': 7,\n",
       " 'abridged': 1,\n",
       " 'form': 49,\n",
       " 'good': 1656,\n",
       " 'info': 44,\n",
       " 'getting': 474,\n",
       " 'down': 649,\n",
       " 'wire': 3,\n",
       " 'apprenticetune': 1,\n",
       " 'cnbc': 57,\n",
       " 'titans': 2,\n",
       " 'shown': 37,\n",
       " 'nov': 21,\n",
       " 'staff': 91,\n",
       " 'sgt': 31,\n",
       " 'salvatore': 1,\n",
       " 'giunta': 1,\n",
       " 'received': 76,\n",
       " 'medal': 29,\n",
       " 'pres': 118,\n",
       " 'obama': 1469,\n",
       " 'month': 77,\n",
       " 'featured': 17,\n",
       " 'delivers': 14,\n",
       " 'appearances': 3,\n",
       " 'isaac': 4,\n",
       " 'mizrahi': 1,\n",
       " 'cathie': 1,\n",
       " 'black': 64,\n",
       " 'si': 19,\n",
       " 'sportsman': 2,\n",
       " 'evanforsi': 1,\n",
       " 'everwere': 1,\n",
       " 'high': 344,\n",
       " 'finaleand': 1,\n",
       " 'liza': 1,\n",
       " 'minnelli': 1,\n",
       " 'brandy': 1,\n",
       " 'clint': 11,\n",
       " 'player': 72,\n",
       " 'leisures': 3,\n",
       " 'worlds': 63,\n",
       " 'five': 71,\n",
       " 'includetrump': 1,\n",
       " 'waikiki': 4,\n",
       " 'walk': 34,\n",
       " 'larrys': 1,\n",
       " 'television': 107,\n",
       " 'farewell': 1,\n",
       " 'jimmy': 27,\n",
       " 'fallon': 12,\n",
       " 'nbcill': 1,\n",
       " 'announcement': 81,\n",
       " 'afternoon': 56,\n",
       " 'speaking': 145,\n",
       " 'neil': 15,\n",
       " 'busy': 35,\n",
       " 'doing': 993,\n",
       " 'phoners': 1,\n",
       " 'wolf': 6,\n",
       " 'blitzer': 4,\n",
       " 'fofriends': 38,\n",
       " 'kudlowcheck': 1,\n",
       " 'january': 56,\n",
       " 'matches': 7,\n",
       " 'factories': 14,\n",
       " 'supposed': 30,\n",
       " 'compete': 24,\n",
       " 'china': 921,\n",
       " 'other': 806,\n",
       " 'countries': 303,\n",
       " 'no': 2299,\n",
       " 'environmental': 19,\n",
       " 'restrictions': 7,\n",
       " 'america': 1695,\n",
       " ...}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##done in the above cell\n",
    "count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iaNdT9dO1GNM"
   },
   "source": [
    "### Part B: Probabilities\n",
    "Now from the counts above we will calculate an associated conditional probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "Xn_rcmtn1SYE"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00000000e+00, 2.30669865e-05, 4.61339731e-05, ...,\n",
       "        0.00000000e+00, 2.30669865e-05, 0.00000000e+00],\n",
       "       [1.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       ...,\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##I made a count transition matrix and normalized it....\n",
    "tmat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PFK8qmFg1o8V"
   },
   "source": [
    "### Part C: Make an Bigram Generator Function\n",
    "Now that we have our probability distrbution, let's make a generator function so that we can generate random Trump tweets using our bigram language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "qe3lugnI1dRS"
   },
   "outputs": [],
   "source": [
    "def get_next_word(state,tmat):\n",
    "    dist = tmat[state]; idx = np.where(dist != 0)[0]\n",
    "    return np.random.choice(idx.tolist(),1,dist[idx].tolist())[0]\n",
    "\n",
    "def generate(start_text='', n=10, tmat=tmat, voc=voc, voci=voci):\n",
    "    \n",
    "    # Helper code to create the start text. \n",
    "    start_text = ['<s>'] + nltk.word_tokenize(preprocess(start_text))\n",
    "    new = []\n",
    "    word = voc[start_text[-1]]\n",
    "    c = 0\n",
    "    while (word != 1) and (c<=n):\n",
    "        word = get_next_word(word, tmat)\n",
    "        c +=1\n",
    "        new.append(word)\n",
    "    new = list(map(lambda x:voci[x],new))\n",
    "    out = start_text + new\n",
    "    return \" \".join(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "_4ssQiWK3VnD"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> i love how dishonest weekly apprentice hardly knew there plans health plan stay'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate('I love')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WC5ZUiZD1-ew"
   },
   "source": [
    "## **(Bonus)** Task 2: Make a Trigram Model\n",
    "Using your code from Parts A-C, see if you can build a trigram (3-gram) model that might make tweets a little more logical. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aY_vnGqB2FxG"
   },
   "source": [
    "#TODO: Your code goes here.\n",
    "countmat3 = np.zeros((nw,nw,nw))\n",
    "for i in range(len(dtraj) - 2):\n",
    "    countmat3[(dtraj[i+2],dtraj[i+1],dtraj[i])] += 1 #column stochastic count matrix for 1 step transitions\n",
    "tmat3 = (countmat3/countmat3.sum(0)).T\n",
    "\n",
    "def get_next_word3(state,tmat):\n",
    "    dist = tmat[state]; idx = np.where(dist != 0)[0]\n",
    "    return np.random.choice(idx.tolist(),1,dist[idx].tolist())[0]\n",
    "\n",
    "def generate3(start_text='', n=10, tmat=tmat, voc=voc, voci=voci):\n",
    "    \n",
    "    # Helper code to create the start text. \n",
    "    start_text = ['<s>'] + nltk.word_tokenize(preprocess(start_text))\n",
    "    new = []\n",
    "    word1 = voc[start_text[-2]]\n",
    "    word2 = voc[start_text[-1]]\n",
    "    c = 0\n",
    "    while (c<=n) and ((word2 and word1) != (0 or 1)):\n",
    "        new = get_next_word((word1,word2), tmat)\n",
    "        word1 = word2\n",
    "        word2 = new\n",
    "        c +=1\n",
    "        new.append(word2)\n",
    "    new = list(map(lambda x:voci[x],new))\n",
    "    out = start_text + new\n",
    "    return \" \".join(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cq3fmX8K_l_N"
   },
   "source": [
    "# Problem 2: Using a Transformer\n",
    "In this section we are going to leverage a pretrained transformer, i.e., [GPT-2](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf), built by the [Hugging Faces Team](https://huggingface.co/gpt2?text=A+long+time+ago%2C). We will also be using their tokenizers because they have been optimized for the language generation task. You should be aware that behind the scenes, their model is using [PyTorch](https://pytorch.org/), the deep learning library built by Facebook, and is quickly becoming more popular than our beloved Tensorflow. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PJNX1muh_7IA"
   },
   "source": [
    "## Task 1: Install and load the GPT2 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "8vf9LGqwfTmd"
   },
   "outputs": [],
   "source": [
    "# TODO: Follow the link above and load the GPT2 model as well as the tokenizer.\n",
    "# NOTE: Replace GPT2LMHeadModel with GPT2LMHeadModel (this is the model for finetuning use cases.)\n",
    "\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "text = \"Replace me by any text you'd like.\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "output = model(**encoded_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "voxq5kGXs3fv"
   },
   "source": [
    "## Task 2: Formatting our Training Data\n",
    "Upcoming we will be fine-tunning this GPT-2 model on our Trump Tweets, but in order to leverage some of the utility classes built by HuggingFaces, we want to take our preprocessed Trump tweets and place them in a flat text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "3ZEdK90df8WG"
   },
   "outputs": [],
   "source": [
    "#TODO: Take our preprocessed trump tweets and write them to a text file\n",
    "with open('trumpdata.txt', 'w') as text_file:\n",
    "    text_file.writelines(preproc)\n",
    "  #TODO: Do something"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "koMncJbEtoyg"
   },
   "source": [
    "## Task 4: Fine-tuning the Model\n",
    "In the following sections, we will complete a couple functions that will allow us to fine-tune the GPT-2 model to our Trump Tweets. I am going to give you a couple of these helper functions, but leave you to write parts of the training function. See the following documentation from Hugging Faces to see the attributes for the [Trainer Class](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.Trainer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "kxJQdEcx3r4g"
   },
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import TextDataset, DataCollatorForLanguageModeling\n",
    "\n",
    "def load_dataset(file_path, tokenizer, block_size = 128):\n",
    "    # Will load and tokenize the data\n",
    "    dataset = TextDataset(\n",
    "        tokenizer = tokenizer,\n",
    "        file_path = file_path,\n",
    "        block_size = block_size,\n",
    "    )\n",
    "    return dataset\n",
    "\n",
    "def load_data_collator(tokenizer, mlm = False):\n",
    "    # Helper Function \n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer, \n",
    "        mlm=mlm,\n",
    "    )\n",
    "    return data_collator\n",
    "\n",
    "def train(\n",
    "    train_file_path,\n",
    "    model,\n",
    "    tokenizer,\n",
    "    output_dir,\n",
    "    overwrite_output_dir,\n",
    "    num_train_epochs,\n",
    "    save_steps\n",
    "    ):\n",
    "  \n",
    "  #TODO: Use the helper functions above to load the dataset and data collector.\n",
    "  global train_dataset\n",
    "\n",
    "  train_dataset = load_dataset(train_file_path,tokenizer)\n",
    "  data_collator = load_data_collator(tokenizer)\n",
    "\n",
    "  # Don't mess with.\n",
    "  tokenizer.save_pretrained(output_dir)\n",
    "  model.save_pretrained(output_dir)\n",
    "\n",
    "  # Don't mess with.\n",
    "  training_args = TrainingArguments(\n",
    "          output_dir=output_dir,\n",
    "          overwrite_output_dir=overwrite_output_dir,\n",
    "          per_device_train_batch_size=8,\n",
    "          num_train_epochs=num_train_epochs,\n",
    "      )\n",
    "  #TODO: Use the Trainer class with the necessary parameters to instaniate the trainer\n",
    "  trainer = Trainer(model,training_args,data_collator, train_dataset, tokenizer,\n",
    "      \n",
    "  )\n",
    "  trainer.train()\n",
    "  #trainer.save()\n",
    "  \n",
    "  #TODO: Train and save the model using the train and save functions built into the Trainer class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "3drTxu5p39K1"
   },
   "outputs": [],
   "source": [
    "#TODO: Set necessary parameters, here are some defaults.\n",
    "train_file_path = \"./trumpdata.txt\"\n",
    "output_dir = 'result'\n",
    "overwrite_output_dir = False\n",
    "num_train_epochs = 1\n",
    "save_steps = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "bLFjEBY44FoD"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading features from cached file ./cached_lm_GPT2Tokenizer_128_trumpdata.txt [took 0.034 s]\n",
      "tokenizer config file saved in result/tokenizer_config.json\n",
      "Special tokens file saved in result/special_tokens_map.json\n",
      "Configuration saved in result/config.json\n",
      "Model weights saved in result/pytorch_model.bin\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running training *****\n",
      "  Num examples = 8800\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1100\n",
      "  Number of trainable parameters = 124439808\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1100' max='1100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1100/1100 33:38, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>4.088500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>3.977300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to result/checkpoint-500\n",
      "Configuration saved in result/checkpoint-500/config.json\n",
      "Model weights saved in result/checkpoint-500/pytorch_model.bin\n",
      "Saving model checkpoint to result/checkpoint-1000\n",
      "Configuration saved in result/checkpoint-1000/config.json\n",
      "Model weights saved in result/checkpoint-1000/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Trainer' object has no attribute 'save'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [64]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#TODO: Use your train function to train the model. It takes about 30 minutes to train in colab.\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_file_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverwrite_output_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnum_train_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_steps\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [60]\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(train_file_path, model, tokenizer, output_dir, overwrite_output_dir, num_train_epochs, save_steps)\u001b[0m\n\u001b[1;32m     49\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(model,training_args,data_collator, train_dataset, tokenizer,\n\u001b[1;32m     50\u001b[0m     \n\u001b[1;32m     51\u001b[0m )\n\u001b[1;32m     52\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m---> 53\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Trainer' object has no attribute 'save'"
     ]
    }
   ],
   "source": [
    "#TODO: Use your train function to train the model. It takes about 30 minutes to train in colab.\n",
    "train(train_file_path, model, tokenizer,output_dir, overwrite_output_dir,num_train_epochs, save_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # The above generates an error because of an accidentally uncommented line,however, the results were saved to disk before the erroneous line :) ...I have since commened out the line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b6MbNciWv0HT"
   },
   "source": [
    "## Task 5: Creating a Tweet Generator\n",
    "Now that we have our trained model, it's time to generate some Tweets. Since we should have saved our model and tokenizer to an output directory, I've already made some helper functions to load those in. We will focus on our *generate_text* function. The function will take as input some start text like \"I am\" or \"My country is\", etc., as well as a max_length parameter which tells the model how much text to generate. Let's Go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "GOXahdOC4I9q"
   },
   "outputs": [],
   "source": [
    "def load_model(model_path):\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "    return model\n",
    "\n",
    "def load_tokenizer(tokenizer_path):\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(tokenizer_path)\n",
    "    return tokenizer\n",
    "\n",
    "def generate_text(sequence, max_length):\n",
    "    #TODO: Load in the finetuned model and tokenizer\n",
    "    model = load_model(\"./result\")\n",
    "    tokenizer = load_tokenizer(\"./result\")\n",
    "\n",
    "    # Encode our passed sequence\n",
    "    ids = tokenizer.encode(sequence,\n",
    "        #TODO:Put something here,\n",
    "        return_tensors='pt'\n",
    "        )\n",
    "    #TODO: Use the generate method in our model class to generate output tokens. \n",
    "    final_outputs = model.generate(ids,max_length = max_length, min_length = 3,\n",
    "        #TODO: Complete some of the parameters and leave the parameters I've passed in.\n",
    "        \n",
    "        # Parameters you should leave\n",
    "        do_sample=True,\n",
    "        pad_token_id=model.config.eos_token_id,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "    )\n",
    "    # Function to print and decode output.\n",
    "    print(tokenizer.decode(final_outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "KC8O6Fnh4X-9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I want to go up in history at the end of my presidency and i think\n"
     ]
    }
   ],
   "source": [
    "#TODO: Now generate some text!\n",
    "generate_text('I want to go', 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ./result/config.json\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"gpt2\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "loading weights file ./result/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at ./result.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "loading file vocab.json\n",
      "loading file merges.txt\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I want to go out of our way to take action to protect the innocent\n"
     ]
    }
   ],
   "source": [
    "generate_text('I want to go', 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ./result/config.json\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"gpt2\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "loading weights file ./result/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at ./result.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "loading file vocab.json\n",
      "loading file merges.txt\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I think that I might be allowed to apologize to my family or to the country if he was ever\n"
     ]
    }
   ],
   "source": [
    "generate_text('I think that I might', 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "provenance": [],
   "toc_visible": true
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
